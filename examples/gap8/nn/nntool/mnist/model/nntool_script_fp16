set input_norm_func "x: x/128-1"
# The quantization does't matter since we want to use float eventually
fquant
qtune * float bfloat16
adjust
fusions --scale8
# set graph_dump_tensor 4
# set graph_checksum 1
set graph_reorder_constant_in true
set graph_produce_node_names true
set graph_produce_operinfos true
set graph_monitor_cycles true

set graph_warm_construct 2

# RAM/FLASH Settings
set l3_ram_device $(MODEL_L3_RAM)
set l3_flash_device $(MODEL_L3_FLASH)

save_state
