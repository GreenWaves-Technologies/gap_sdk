# Key Word Spotting including 4 mikes  capture through I2S interface and front end audio processing

## Introduction
This test performs 4 mikes audio capture with I2S interface on the Gapuino board equipped with the sensor board including the 4 mikes.
The captured signal is processed by a VAD. When voice signal is detected, a 1 second audio acquisition is performed, with beamforming and noise reduction. A third partie IP is available on a compiled form to perform VAD and beamforming.
==> PLEASE NOTE that the IP has a time limitation implemented. After 2 minutes, the IP will cease functioning, and the signal acquisition will be blocked.

The captured signal Tke word is first processed by  MFCC (Mel Frequency Cepstrum COefficient)  to produce a time frequency image of the input signal. Then a 3 layers neural network is applied to recognize the learned words.

The application uses the MBED operating system.

Two applications can be built:
The KWS (KeyWord Spotting), where a list of 10 words can be recognized.
The "alexa" application, where alexa wakeup word is recognized.

Please note that this is a demo code. It shows how to implement a functionally correct audio acquisition and processing using GAP8. The performance (of the complete chain (capture+front end+CNN) is less than the CNN part only. Some words in the KWS list are badly recognized (notably "off") due some distorsion introduced by the preprocessing
Also in the alexa mode, false positive may occur. There is certainly room for improvement.

## What is needed
- gapuino board v1 or v2
- sensor board
- gap_sdk v2 tool chain


## Sound signal capture

The sound is captured by the 4 mics array of the sensor board (include picture).
Each mike has a PDM (Pulse Density Modulation) digital output. 2 mikes are connected to one I2S interface (I2S0), and the 2 others to the second I2S interface (I2S1), each configured in dual data rate (DDR) mode.

The are sampled at 1,024Mhz, using either a clock generated by GAP8 (internal clock divided by a programmable factor), or by  an external oscillator (the example is configured to use the GAP8 produced clock). 
The DDR PDM bitsrtream from one I2S input is downsampled by a decimation filter down to the 16Khz input signal sampling frequency, with 16 bits resolution.

Note that other sampling frequency can be selected for the mikes by adjusting the division factor in case the internal clock is used. In this case, the decimation factor has to be adjusted accordingly to ensure that the output signal is sampled at 16Khz.
By example, for 1.024Mhz PDM bitrate, the factor must be set to 64 (1,024Mhz/64 = 16Khz).

The dynamic of the signal at the output of the decimator is programmable:
The decimator accumulates samples in a 51 bits register.
Out of these 51bits, a 16 bits word is extracted. The parameter SHIFT_CHAN0 for I2S0 (resp SHIFT_CHAN1 for I2S1) specifies the right shift to do on the 51 bits register to get the 16 bits. The result is not clipped, so in case of saturation, the signal may incur a transition from +A to -A, and create spurious in the voice signal.

## Audio front end

In the application, the 4 voice streams from the mikes are processed by a third party Intellectual Property (IP). THis IP performs voice activity detection (VAD), beamforming and noise reduction. THe signal is processed by chunks of 128 samples.

VAD: when the voice signal is above a certain threshold on a chunk, a flag is set to one, triggering beamforming, noise reduction and the KWS analysis.
Beamforming : The phase difference between the signals from the 4 mikes is analysed to evaluate the direction of the main sound source. Using adaptive filtering, the sound capture will be focused on the source, reducing significantly the influence of other less significant sources.
Noise reduction: with some assumption on the statistic of the noise, adaptive filtering is performed to reduce further the noise level in the signal.

The output of the IP is a 16 Khz stream of 16 bits data that will be further analyzed by the KWS. 
Once the VAD is active, a 1sec buffer of speech signal is recorded for analysis by the KWS.

## KWS algorithm

The KWS has been ported from the floating point python version of the speech_command tutorial from Google: (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands.  ).

For training of the CNN has been done in Tensorflow, using the V0.2 dataset from Pete Warden (described in the paper  arXiv:1804.03209v1) is used.
The complete test set is stored in http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz.
The translation of the tensorflow CNN to GAP8 code, including quantization of the weights and biases has been performed using the TF2GAP8 (Tensorflow to GAP8) automatic bridge developed by Greenwaves. This bridge uses the autotiler code generator to insure the most efficient use of the compute resources of GAP8.


Since GAP8 has no floating point unit, the original MFCC has been quantized from floating point to 32b/16b fixed point arithmetic. 

The performance of the quantized KWS on GAP8 has been measured independently of the front end interface by injecting voice waveforms from the test dataset provided with the V0.2 dataset directly in the GAP8 L2 memory and performing inference.

The measured recognition ratio is 95.5%, which is very close to the 96% measured on the trained floating point model.

The MFCC is executed on the Fabric Controller (execution on a single core), the CNN is executed on the cluster using either the 8 cores or the hardware accelerator.

The 3 layers CNN is comprised of 2 convolution layers, and one fully connected layer.
- first conv layer: input is image 40x98 cepstrum coefficients
  output is 32 x 16x 40 images. Filter kernels are 8x20 (32x8x20 coefficients).
- The second layer produces 32 x 13 x 30 images, convolution kernels are 4x10 (32x4x10 coefficients)
- The last layer is fully connected producing 12 outputs (12x32x13x30 coefficients)



## What in this folder:

| Name          |         Description                                              |
|---------------|------------------------------------------------------------------|
|Makefile       |  Makefile of this example                                        |
|main.c    	|  Source code of this test                                        |
|KWS		|  The libraries for MFCC and CNN				   |
|README.md      |  This readme file                                                |
|vocal_timelimit.lib| the library of audio front end (VAD, beam forming)	   |


## How to build and execute the application
## int the gap_sdk installation directory
source  sourceme.sh

To run the example using the board:

~~~~~sh
make clean all run
~~~~~


If you want to build the KWS or ALEXA  application, just uncomment the relevant MBED_FLAG line in the Makefile.

## measurement of the correct detection ratio on the reference test set

An iterative test is available. It loads .wav waveforms from the dataset converts to an .h file and runs the kws program on GAP8.A partial dataset is available in "reduced_test_set" directory

It measures the number of correct detections over a reduced set of data.

to run the test, do:

> source ./test.sh  ./reduced_test_set/testing_list_small_small.txt 0

~~~~~
